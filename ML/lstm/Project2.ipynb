{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make sure you fill in all cells contain YOUR CODE HERE or YOUR ANSWER HERE.\n",
    "2. After you finished, Restart the kernel & run all cell in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project II: Text Classification Using LSTM Network\n",
    "## Deadline: Nov 14, 11:59 pm\n",
    "\n",
    "You have learned about the basics of neural network training and testing during the class. Now let's move forward to the text classification tasks using simple LSTM networks! In this project, you need to implement two parts:\n",
    "\n",
    "- **Part I: Building vocabulary for LSTM network**\n",
    "    - Get familiar with discrete text data processing for neural networks. Building vocabulary by yourself.\n",
    "\n",
    "\n",
    "- **Part II: Implementing your own LSTM Neural Network**\n",
    "    - Learn to implement your own LSTM network and aims for a strong performance on the given text classification task.\n",
    "    - Note that you need to implement the LSTM network manually, any kind of integrated package invoking will get 0 points.\n",
    "    - Your LSTM network can be 2-4 layers.\n",
    "    - Expected Accuracy: >=65%.\n",
    "    ![](./LSTM.png)\n",
    "    \n",
    "\n",
    "项目二:利用LSTM网络进行文本分类\n",
    "截止日期:11月14日晚上11:59\n",
    "您已经在课堂上学到了神经网络训练和测试的基础知识。现在让我们前进到使用简单的LSTM网络的文本分类任务！在这个项目中，您需要实现两个部分:\n",
    "\n",
    "第一部分:为LSTM网络建立词汇\n",
    "熟悉神经网络的离散文本数据处理。自己建立词汇。\n",
    "第二部分:实现你自己的LSTM神经网络\n",
    "学习实现你自己的LSTM网络，目标是在给定的文本分类任务上有很好的表现。\n",
    "请注意，您需要手动实现LSTM网络，任何类型的集成包调用将获得0分。\n",
    "你的LSTM网络可以有2-4层。\n",
    "预期精度:> =65%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# nlp library of Pytorch\n",
    "# from torchtext import data\n",
    "from torchtext.legacy import data\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "SEED = 2021\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cuda.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    5574 non-null   object\n",
      " 1   text    5574 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data_ = pd.read_csv('./sms_spam.csv')\n",
    "data_.head()\n",
    "data_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field is a normal column \n",
    "# LabelField is the label column.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]\n",
    "\n",
    "TEXT = data.Field(tokenize=tokenizer,batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(\"type\",LABEL),('text',TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'ham', 'text': ['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']}\n"
     ]
    }
   ],
   "source": [
    "training_data = data.TabularDataset(path=\"./sms_spam.csv\",\n",
    "                                    format=\"csv\",\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True\n",
    "                                   )\n",
    "\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# train and validation splitting\n",
    "train_data,valid_data = training_data.split(split_ratio=0.75,\n",
    "                                            random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (5 points)\n",
    "Implement the vocabulary building and the text to label part for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Question1 here:\n",
    "#Building vocabularies => (Token to integer)\n",
    "#you can use the data package built-in function to build the vocabulary, check the 'torchtext data' doc.\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text vocab: 9705\n",
      "Size of label vocab: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 3658),\n",
       " ('to', 1615),\n",
       " ('I', 1478),\n",
       " (',', 1461),\n",
       " ('you', 1383),\n",
       " ('?', 1086),\n",
       " ('!', 1019),\n",
       " ('a', 1003),\n",
       " ('the', 882),\n",
       " ('...', 869)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Size of text vocab:\",len(TEXT.vocab))\n",
    "print(\"Size of label vocab:\",len(LABEL.vocab))\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# We'll create iterators to get batches of data when we want to use them\n",
    "\"\"\"\n",
    "This BucketIterator batches the similar length of samples and reduces the need of \n",
    "padding tokens. This makes our future model more stable\n",
    "\n",
    "\"\"\"\n",
    "train_iterator,validation_iterator = data.BucketIterator.splits(\n",
    "    (train_data,valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    # Sort key is how to sort the samples\n",
    "    sort_key = lambda x:len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (25 points)\n",
    "You need to implement the embedding layer and the LSTM cell according to the given architecture, but you are not allowed to use any integrated package!\n",
    "LSTM tutorial: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "![](./LSTM_CELL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9ce29919db60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paddle'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class lstm_(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim,n_layers=2,bidirectional=False,bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = embedding_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.bias = bias\n",
    "        self.num_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2 \n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        self.param_names = []\n",
    "        for layer in range(self.num_layers):\n",
    "            self.param_names.append([])\n",
    "            for direction in range(self.num_directions):\n",
    "                self.input_size = self.input_size if layer == 0 else self.hidden_size * self.num_directions\n",
    "                W =  Parameter(torch.Tensor([self.input_size, self.hidden_size * 4]))\n",
    "                U =  Parameter(torch.Tensor([self.hidden_size, self.hidden_size * 4]))\n",
    "                b =  Parameter(torch.Tensor([self.hidden_size * 4]))\n",
    "                layer_params = (W, U, b)\n",
    "                suffix = '_reverse' if direction == 1 else ''\n",
    "                self.param_name = ['weight_W{}{}', 'weight_U{}{}']\n",
    "                if bias:\n",
    "                    self.param_name += ['bias_{}{}']\n",
    "                self.param_name = [x.format(layer, suffix) for x in self.param_name]\n",
    "                for name, param in zip(self.param_name, layer_params):\n",
    "                    setattr(self, name, param)\n",
    "                self.param_names[layer].append(self.param_name)\n",
    "\n",
    "            \n",
    "\n",
    "        self.all_weights = [[[getattr(self, weight) for weight in weights]\n",
    "                        for weights in weights_layer] for weights_layer in self.param_names]\n",
    "\n",
    "    def _init_states(self, x):\n",
    "        h_t = torch.zeros(1, x.size(0), self.hidden_size, dtype=x.dtype).to(x.device)\n",
    "        c_t = torch.zeros(1, x.size(0), self.hidden_size, dtype=x.dtype).to(x.device)\n",
    "        return h_t, c_t\n",
    "\n",
    "    def forward(self, x,init_states=None):\n",
    "\n",
    "        batch_size, seq_sz, _ = x.shape\n",
    "        if init_states is None:\n",
    "            num_directions = 2 if self.bidirectional else 1\n",
    "            h_t, c_t = (torch.zeros((self.num_layers * num_directions,batch_size,self.hidden_size)),\n",
    "                        torch.zeros((self.num_layers * num_directions,batch_size,self.hidden_size)))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden_seq = []\n",
    "            hidden_seq_reverse = []\n",
    "            self.weight_layer = self.all_weights[layer]\n",
    "            for direction in range(self.num_directions):\n",
    "                # self.param_name = self.param_names[layer]\n",
    "                self.weight = self.weight_layer[direction]\n",
    "                HS = self.hidden_size\n",
    "                h_t, c_t = h_t[0].unsqueeze(0),c_t[0].unsqueeze(0)\n",
    "                for t in range(seq_sz):\n",
    "                    x_t = x[:, t, :]\n",
    "                    # batch the computations into a single matrix multiplication\n",
    "                    print(self.weight[0])\n",
    "#                     gates = x_t @ getattr(self,self.param_name[0]) + h_t @ getattr(self,self.param_name[1]) \\\n",
    "#                        + getattr(self,self.param_name[2])\n",
    "#                     print(self.weight[1].shape)\n",
    "#                     print(x_t.shape)\n",
    "                    if self.bias:\n",
    "                        gates = x_t @ self.weight[0] + h_t @ self.weight[1] + self.weight[2]\n",
    "                    else:\n",
    "                        gates = x_t @ self.weight[0] + h_t @ self.weight[1]\n",
    "\n",
    "                    gates = gates[0]\n",
    "                    i_t, f_t, g_t, o_t = (\n",
    "                        paddle.nn.functional.sigmoid(gates[:, :HS]),  # input\n",
    "                        paddle.nn.functional.sigmoid(gates[:, HS:HS * 2]),  # forget\n",
    "                        paddle.tanh(gates[:, HS * 2:HS * 3]),\n",
    "                        paddle.nn.functional.sigmoid(gates[:, HS * 3:]),  # output\n",
    "                    )\n",
    "                    c_t = f_t * c_t + i_t * g_t\n",
    "                    h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "                    if direction == 0:\n",
    "                        if isinstance(hidden_seq, list):\n",
    "                            hidden_seq = h_t[0].unsqueeze(1)\n",
    "                        else:\n",
    "                            hidden_seq = paddle.concat((hidden_seq, h_t[0].unsqueeze(1)), axis=1)\n",
    "\n",
    "                    if direction == 1:\n",
    "                        if isinstance(hidden_seq_reverse, list):\n",
    "                            hidden_seq_reverse = h_t[0].unsqueeze(1)\n",
    "                        else:\n",
    "                            hidden_seq_reverse = paddle.concat((hidden_seq_reverse, h_t[0].unsqueeze(1)), axis=1)\n",
    "                x = paddle.to_tensor(x.detach().cpu().numpy()[:,::-1,:].copy())\n",
    "                if direction == 1:\n",
    "                    hidden_seq_reverse = paddle.to_tensor(\n",
    "                        hidden_seq_reverse.detach().cpu().numpy()[:, ::-1, :].copy())\n",
    "                    hidden_seq = paddle.concat((hidden_seq, hidden_seq_reverse),axis=2)\n",
    "            x = hidden_seq\n",
    "        return hidden_seq, (h_t, c_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout):\n",
    "        \n",
    "        super(LSTMNet,self).__init__()\n",
    "        # In this class, you need to implement the architecture of an LSTM network, the architecture should include:\n",
    "        # 1. Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim,padding_idx=0)        \n",
    "        # 2. LSTM layer process the vector sequences \n",
    "        self.lstm = lstm_(embedding_dim, hidden_dim,n_layers,bidirectional=bidirectional)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        # self.u = nn.Parameter(torch.Tensor(config.hidden_size * 2, config.hidden_size * 2))\n",
    "        self.w = nn.Parameter(torch.zeros(hidden_dim * 2))\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.act = F.sigmoid()\n",
    "    def forward(self, x,x_len):\n",
    "        emb = self.embedding(x)  # [batch_size, seq_len, embeding]=[128, 32, 300]\n",
    "        H, _ = self.lstm(emb)  # [batch_size, seq_len, hidden_size * num_direction]=[128, 32, 256]\n",
    "\n",
    "        M = self.tanh1(H)  # [128, 32, 256]\n",
    "        # M = torch.tanh(torch.matmul(H, self.u))\n",
    "        alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)  # [128, 32, 1]\n",
    "        out = H * alpha  \n",
    "        out = torch.sum(out, 1)  \n",
    "        out = self.fc(out) \n",
    "        out = self.act(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_HIDDEN_NODES = 64\n",
    "NUM_OUTPUT_NODES = 1\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTION = True\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(SIZE_OF_VOCAB,\n",
    "                EMBEDDING_DIM,\n",
    "                NUM_HIDDEN_NODES,\n",
    "                NUM_OUTPUT_NODES,\n",
    "                NUM_LAYERS,\n",
    "                BIDIRECTION,\n",
    "                DROPOUT\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimizer,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # cleaning the cache of optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text,text_lengths = batch.text\n",
    "        \n",
    "        # forward propagation and squeezing\n",
    "        predictions = model(text,text_lengths).squeeze()\n",
    "        # computing loss / backward propagation\n",
    "        loss = criterion(predictions,batch.type)\n",
    "        loss.backward()\n",
    "        \n",
    "        # accuracy\n",
    "        acc = binary_accuracy(predictions,batch.type)\n",
    "        # updating params\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # It'll return the means of loss and accuracy\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    # deactivate the dropouts\n",
    "    model.eval()\n",
    "    \n",
    "    # Sets require_grad flat False\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text,text_lengths).squeeze()\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.type)\n",
    "            acc = binary_accuracy(predictions, batch.type)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUMBER = 15\n",
    "for epoch in range(1,EPOCH_NUMBER+1):\n",
    "    \n",
    "    train_loss,train_acc = train(model,train_iterator,optimizer,criterion)\n",
    "    \n",
    "    valid_loss,valid_acc = evaluate(model,validation_iterator,criterion)\n",
    "    \n",
    "    # Showing statistics\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
